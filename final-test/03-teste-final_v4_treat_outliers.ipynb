{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem: \n",
    "### Predicting whether a person with a given set of characteristics is likely to have a heart attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "!pwd\n",
    "df_raw = pd.read_csv(\"final-test/data/heart.csv\") \n",
    "print(\"Columns:\", df_raw.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_raw.head(1))\n",
    "print(\"Types :\\n\",df_raw.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_raw.isna())\n",
    "print(\"SIZE: \", len(df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same the df.shape[0]\n",
    "df_raw.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado: não existens ausentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# Normalization\n",
    "# Apply StandardScaler ONLY to numeric columns.\n",
    "# And you should NEVER try to transform categorical columns to 0 and 1 before standardizing.\n",
    "# The same logic applies to OneHotEncoder (categorical columns ONLY)\n",
    "\n",
    "df_categorical = df_raw.select_dtypes(exclude=['number'])\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded = encoder.fit_transform(df_categorical)\n",
    "\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded,\n",
    "    columns=encoder.get_feature_names_out(df_categorical.columns),\n",
    "    index=df_raw.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers: Statistical Rule vs. Quality Rule (Domain Violation)\n",
    "Fundamental Difference (Very Important)\n",
    "\n",
    "## Statistical Outlier (IQR)\n",
    "- Extreme but plausible value\n",
    "- Ex.: Cholesterol = 450\n",
    "- Action: cap / flag / feature engineering\n",
    "\n",
    "## Domain Violation (Data Quality)\n",
    "- Impossible value in the real world\n",
    "- Ex.: Age = 200\n",
    "- Action: quality rule, not statistical\n",
    "\n",
    "*Age = 200 is not an outlier\n",
    "\n",
    "*Age = 200 is invalid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treat outliers only in the original numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df_raw.select_dtypes(include=['number'])\n",
    "df_numeric.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_numeric[\"RestingBP\"].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_numeric[\"FastingBS\"].value_counts()) ## is a binary variable; statistical IQR does not apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_numeric[\"Age\"].value_counts()) ## It is already numerical not to apply statistical IQR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do with these cases?\n",
    "\n",
    "It depends on the project's maturity:\n",
    "\n",
    "Option When to use Correct (cap at 120) legacy data Input few occurrences \n",
    "\n",
    "Delete critical error record Block ingestion from mature pipelines Create flag always recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"HeartDisease\"\n",
    "cols_numeric_features = [c for c in df_numeric.columns if c != target_col]\n",
    "\n",
    "df_outliers = df_numeric.drop(columns=[\"HeartDisease\"],  errors=\"ignore\")\n",
    "\n",
    "# Automatically detects binary characters (e.g., 0/1). Then we should not apply the IRQ.\n",
    "binary_cols = [c for c in cols_numeric_features if df_outliers[c].dropna().nunique() <= 2] \n",
    "\n",
    "# continuous (where IQR makes sense)\n",
    "cols_iqr = [c for c in cols_numeric_features if c not in binary_cols and c not in ['Age']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cols_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" OLD check outliers statistic \"\"\"\n",
    "\n",
    "## data removed\n",
    "for col in df_outliers.columns:\n",
    "    Q1 = df_raw[col].quantile(0.25)\n",
    "    Q3 = df_raw[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df_raw[(df_raw[col] < lower) | (df_raw[col] > upper)][col]\n",
    "    print(f\"{col}: {len(outliers)} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NEW FUNCTION OUTLIERS \"\"\"\n",
    " \n",
    " # created new columns  as col_outlier_flag \n",
    "import numpy as np\n",
    "\n",
    "def iqr_cap_with_flag(df, cols, k=1.5):\n",
    "    df2 = df_outliers.copy()\n",
    "    limits = {}\n",
    "\n",
    "    for col in cols:\n",
    "        q1 = df2[col].quantile(0.25)\n",
    "        q3 = df2[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        low = q1 - k * iqr\n",
    "        high = q3 + k * iqr\n",
    "\n",
    "        limits[col] = (low, high)\n",
    "\n",
    "        df2[f\"{col}_outlier_flag\"] = ((df2[col] < low) | (df2[col] > high)).astype(int)\n",
    "        df2[f\"{col}_capped\"] = df2[col].clip(lower=low, upper=high)\n",
    "\n",
    "    return df2, limits\n",
    "\n",
    "df_numeric_treated, iqr_limits = iqr_cap_with_flag(df_outliers, cols=cols_iqr, k=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric_treated.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_numeric_treated\n",
    "\n",
    "Interpretation of maturity (high level)\n",
    "O teu dataset agora separa claramente:\n",
    "| Tipo     | Exemplo     | Tratamento       |\n",
    "| -------- | ----------- | ---------------- |\n",
    "| Domínio  | Age         | regra de negócio |\n",
    "| Binário  | FastingBS   | validação lógica |\n",
    "| Contínuo | Cholesterol | IQR + cap + flag |\n",
    "\n",
    "This mean Data Quality + Feature Engineering, it not just ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all variables\n",
    "df_encoded = pd.concat([df_numeric_treated, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "df_encoded.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds/returns the target variable\n",
    "df_encoded[\"HeartDisease\"] = df_raw[\"HeartDisease\"]\n",
    "df_encoded.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[\"HeartDisease\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Checking with IQR\n",
    "\n",
    "IQR = Q3 – Q1\n",
    "- Where:\n",
    "- Q1 = 25th percentile\n",
    "- Q3 = 75th percentile\n",
    "\n",
    "A value is considered an outlier if it falls outside of: [ Q1 - 1.5*IQR , Q3 + 1.5*IQR ]\n",
    "\n",
    "Important Rule: Outliers only make sense for:\n",
    "\n",
    "original continuous numeric variables\n",
    "- Never for one-hot columns (0/1)\n",
    "-  Never for the target\n",
    "\n",
    "Mental Summary (important):\n",
    "- df_encoded is the correct DataFrame\n",
    "- Outliers only in real continuous variables\n",
    "-  Do not apply IQR to one-hot columns\n",
    "-  Do not apply IQR to the target\n",
    "- Save iqr_limits for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique -> RestingBP\n",
    "I will use the most recommended pattern: Winsorization (cap) + flag only for continuous numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_encoded[\"RestingBP\"].value_counts()) ### applied to the function: iqr_cap_with_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_encoded[\"Cholesterol\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_encoded[\"MaxHR\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_encoded[\"Oldpeak\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of outlier analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~Remove Outliers~~ We handle outliers in original numeric columns, not binary ones.\n",
    "Removing outliers reduces the number of rows in the dataset. If you do this for many columns and have a narrow IQR, you may lose a lot of data. Alternative: replace outliers with medians or limits, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_clean)) ## old, there were 587 rows left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_encoded)) # manteve 918 linhas mesmo tamanho de df_raw OK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not executed after outlier treatment, only before and after check\n",
    "## Check total record: BEFORE TREATING OUTLIERS\n",
    "\n",
    "print(\"Total: \",df.shape[0],\"Total sem outliers: \", df_clean.shape[0])\n",
    "reduced = df.shape[0] - df_clean.shape[0]\n",
    "print(f'A total of {reduced} were reduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate the data into training/test sets with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "y = df_encoded[\"HeartDisease\"]\n",
    "X = df_encoded\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_cols = X_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply a K-Nearest Neighbors (KNN) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model using:\n",
    "\n",
    "- Confusion matrix\n",
    "- Accuracy, Precision, Recall, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## predict: predictions with test data:\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")    \n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "TN (True Negative): The model predicted 0 and the actual label was 0 → correct prediction of negative.\n",
    "\n",
    "FN (False Negative): The model predicted 0 but the actual label was 1 → \"detection failure\".\n",
    "\n",
    "FP (False Positive): The model predicted 1 but the actual label was 0 → \"false alarm\".\n",
    "\n",
    "TP (True Positive): The model predicted 1 and the actual label was 1 → correct prediction of positive.\n",
    "\n",
    "\n",
    "| Real \\ predicted | 0 (predicted) | 1 (predicted) |\n",
    "|-----------------|--------------|--------------|\n",
    "| 0 (Real)        | TN           | FP           |\n",
    "| 1 (Real)        | FN           | TP           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Classification Report: \\n\" , classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLANATION 1\n",
    "### After oneHotEncoder\n",
    "### Only with numeric variables\n",
    "\n",
    "# KNN Analysis (based on pdf 06-FT03)\n",
    "``` knn = KNeighborsClassifier(n_neighbors=5) ```\n",
    "\n",
    "In the first application with default hyperparameters and a cluster count of 5, we obtained:\n",
    "\n",
    "**Total sample size: 918 rows (100%)**\n",
    "\n",
    "- TN (True Negative): 76% \n",
    "- FP (False Positive): 6% \n",
    "- FN (False Negative): 3% \n",
    "- TP (True Positive): 99% \n",
    "\n",
    "\n",
    "In other words, more than 175% of the total TN + TP (76% TN + 99% TP) of our model trained correctly.\n",
    "\n",
    "Conversely, we had 3 (FN) lines of the sample incorrectly classified; in the case of this problem, which could be fatal, we will consider it a failure in disease detection (FN) by the model.\n",
    "\n",
    "As an area for improvement: reduce the 'False Negatives'.\n",
    "\n",
    "* Regarding other metrics:\n",
    "\n",
    "\n",
    "1. Accuracy: We obtained *0.936*, which we can consider excellent for the model, as it managed to predict 94% of the cases in the sample. However, it is important to adjust it to include the remaining 3% (FN) as TP.\n",
    "\n",
    "Best parameters: {'knn__metric': 'manhattan', **'knn__n_neighbors': 11**, 'knn__weights': 'distance'}\n",
    "Best accuracy (validation): 0.936\n",
    "Accuracy in the test: 0.951\n",
    "\n",
    "\n",
    "\n",
    "| Classe | Precision | Recall | F1-score | Support |\n",
    "|--------|-----------|--------|----------|---------|\n",
    "| 0      | 0.96      | 0.93   | 0.94     | 82      |\n",
    "| 1      | 0.94      | 0.97   | 0.96     | 102     |\n",
    "| **Accuracy** |           |        | **0.95** | 184     |\n",
    "| **Macro avg** | 0.95      | 0.95   | 0.95     | 184     |\n",
    "| **Weighted avg** | 0.95      | 0.95   | 0.95     | 184     |\n",
    "\n",
    "Healthy people:\n",
    "2. In the accuracy of positives and correct results, we had 96%, of which 4% are classified as imprecise, not 100% 'certain', such as 0 (they may or may not be healthy). In this case, they were marked as healthy but may be <u>sick</u>.\n",
    "\n",
    "3. Recall = True Positives / (True Positives + False Negatives)\n",
    "This means that of all the examples that were actually class 0, healthy people, the model was able to identify 93%, leaving only 7% as falsely healthy.\n",
    "\n",
    "4. F-score\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "This means that this metric measures balance. It prevents a model with high recall and low precision (or vice versa) from looking good.\n",
    "In short, we need to have both metrics balanced to actually have a model with good prediction.\n",
    "f.Score = 94% and recall = 93% (we have balance)\n",
    "\n",
    "Averages:\n",
    "Macro Avg = 0.95 may represent an optimal model.\n",
    "Weighted Avg = 0.95, similar to accuracy, therefore, we can say that the classes are balanced among themselves.\n",
    "➡️ If weighted = accuracy, normally the classes are not very unbalanced.\n",
    "Weighted Avg = 95% and comparing with accuracy = 94% (we have balance)\n",
    "\n",
    "In conclusion, we found a critical area for potential future improvement:\n",
    "\n",
    "1. Critical point for improvement, for recall in the class of sick people, positive for attack. We had a recall of 94%, meaning it failed to correctly predict 6% of the sick individuals, which can be fatal.\n",
    "\n",
    "1.1 Suggested solution: improve the balancing of sick people (this was done, we applied OneHotEncoder), that is, increase the number of sick people in the model's training sample. To establish patterns for sick people, we need to confirm if we have more attributes that can better explain class 1 (sick people). DONE\n",
    "\n",
    "1.2 We handled outliers with flag techniques, only treating original numeric variables from df_raw and not binary ones. DONE\n",
    "\n",
    "Result: after improvements 1.1 and 1.2, the TP increased to 99% detection. The suggestion for a new application of KNN will be with 11K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~EXPLANATION 2~~ before oneHotEncoder\n",
    "### Before oneHotEncoder\n",
    "### With numeric + categorical variables transformed into numeric variables using OneHotEncoder\n",
    "# KNN Analysis (based on pdf 06-FT03)\n",
    "``` knn = KNeighborsClassifier(n_neighbors=5) ```\n",
    "\n",
    "In the first application with standard hyperparameters and a cluster count of 5, we obtained:\n",
    "TP (True Positive): 59 out of a total of 587 samples\n",
    "TN (True Negative): 16 out of a total of 587 samples\n",
    "FP (False Positive): 19 out of a total of 587 samples\n",
    "FN (False Negative): 31 out of a total of 587 samples\n",
    "\n",
    "That is, more than 50% of our model trained correctly. On the other hand, we had 31 lines of the sample incorrectly classified; in the case of this problem, it can be fatal, so we will consider it a failure in the model's disease detection.\n",
    "\n",
    "Regarding other metrics:\n",
    "1. Accuracy: we obtained *0.755*, which we can consider good for the model, as it managed to predict 70% of the cases in the sample. However, it is important to adjust it to include the remaining 30% as TP.\n",
    "\n",
    "Best accuracy (validation): 0.755\n",
    "Accuracy in the test: 0.703\n",
    "\n",
    "Healthy people:\n",
    "\n",
    "| Classe        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| **0**         | 0.73      | 0.76   | 0.75     | 68      |\n",
    "| **1**         | 0.66      | 0.62   | 0.64     | 50      |\n",
    "| **Accuracy**  | —         | —      | 0.70     | 118     |\n",
    "| **Macro Avg** | 0.70      | 0.69   | 0.69     | 118     |\n",
    "| **Weighted Avg** | 0.70  | 0.70   | 0.70     | 118     |\n",
    "\n",
    "2. In the Precision of positives, we had 73%.\n",
    "\n",
    "3. Recall\n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "This means that of all the examples that were actually class 0, healthy people, the model managed to identify 76%, leaving only 24% as false negatives.\n",
    "\n",
    "4. F-score\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "This means that this metric measures balance. It prevents a model with high recall and low precision (or vice versa) from appearing good.\n",
    "\n",
    "In short, we need to have both metrics balanced to actually have a model with good prediction.\n",
    "\n",
    "Averages:\n",
    "Macro Avg = 0.69 may represent a moderate model.\n",
    "Weighted Avg = 0.70, similar to accuracy, therefore, we can say that the classes are balanced between them.\n",
    "\n",
    "➡️ If weighted = accuracy, then classes are usually not very unbalanced.\n",
    "\n",
    "Conclusion\n",
    "We found a critical point for possible future improvement:\n",
    "\n",
    "1. Critical point for improvement, for recall in the class of sick people, positive for attack. We had a recall = 62%, meaning it failed to correctly predict 38% of the sick people, which can be fatal.\n",
    "\n",
    "1.1 Suggested solution: improve the balancing of sick people, that is, increase the number of sick people in the model's training sample. As for the patterns of sick people, we need to confirm if we have more attributes that can better explain class 1 (sick people).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizar o número de vizinhos com GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling Pipeline\n",
    "\n",
    "### Step 1 — StandardScaler\n",
    "- Normalizes all features\n",
    "- Assumes all are numerical\n",
    "- Sets mean = 0 and standard deviation = 1\n",
    "\n",
    "### Step 2 — KNeighborsClassifier\n",
    "- Highly scale-sensitive model\n",
    "- Distances dominate the decision\n",
    "\n",
    "#### When is this pipeline appropriate?\n",
    "\n",
    "- Fully numeric dataset\n",
    "- PCA, KNN, K-Means, SVM\n",
    "- Clean data without categorical variables\n",
    "- Educational or exploratory notebooks\n",
    "\n",
    "##### When is this pipeline inadequate?\n",
    "\n",
    "- Dataset with categorical columns\n",
    "- Mixed data (num + cat)\n",
    "- Production\n",
    "- Advanced feature engineering\n",
    "\n",
    "##### In this  case:\n",
    "\n",
    ">> It wouldn't be sufficient\n",
    "\n",
    ">  It would break the semantics of categorical variables\n",
    "\n",
    ">  It would generate errors or incorrect learning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Feature Scaling Pipeline -> This pipeline is a didactic example of a feature scaling application, suitable only when all variables are numeric.\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Hiper parametros\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": [3, 5, 7, 9, 11],\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    \"knn__metric\": [\"euclidean\", \"manhattan\"]\n",
    "}\n",
    "\n",
    "# otimização de vizinhos\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Melhores parâmetros:\", grid.best_params_)\n",
    "print(f\"Melhor accuracy (validação): {grid.best_score_:.3f} \")\n",
    "\n",
    "# best_model = grid.best_estimator_\n",
    "# y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# accuracy_score: percentagem de classificações corretas\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy no teste: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparar o desempenho antes e depois da otimização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de KNN (baseado no pdf 06-FT03)\n",
    "## knowledge\n",
    "``` knn = KNeighborsClassifier(n_neighbors=5) ```\n",
    "\n",
    "Na primeira aplicação com hiper parametros padrão e número de clusters igual 5, obtivemos:\n",
    "TP (True Positive): 59 do total de 587 amostras\n",
    "TN (True Negative): 16 do total de 587 amostras\n",
    "FP (False Positive): 19 do total de 587 amostras\n",
    "FN (False Negative): 31 do total de 587 amostras\n",
    "\n",
    "ou seja mais de 50% do nosso modelo treinou corretamente. Em contra partida, tivemos 31 linhas da amostra classificado incorretamente, no caso do problema em questão pode ser fatal, vamos considerar como falha na deteção da doença pelo modelo.\n",
    "\n",
    "Com relação a outras métricas:\n",
    "1. Accuracy: obtivemos *0.755*, podemos considerar  bom para o modelo conseguiu predizer 70 % dos casos na amostra, porém é importate ajustar para conseguir adicionar os 30% restantes como TP.\n",
    "\n",
    "Pessoas saudáveis:\n",
    "2. Na Precisão de posotivos e está certo tivemos 73 %\n",
    "\n",
    "3. recall\n",
    "Recall = Verdadeiros Positivos / (Verdadeiros Positivos + Falsos Negativos)\n",
    "O que significa que todos os exemplos que realmente eram classe 0, pessoas saudáveis, o modelo conseguiu identificar 76%. deixando apenas 24% como falsos doentes.\n",
    "\n",
    "4. F-score\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "o que significa que essa métrica mede o equilíbrio. Evita que um modelo com recall alto e precision baixa (ou vice-versa) pareça bom.\n",
    "em resumo, precisamos ter as duas métricas balanceadas para de facto ter um modelo com boa predição.\n",
    "\n",
    "Médias:\n",
    "Macro Avg = 0.69 pode representar um modelo moderado.\n",
    "Weighted Avg = 0.70, semelhante a accuracy, logo, podemos dizer que as classes estão balanceadas entre elas.\n",
    "➡️ Se weighted = accuracy, normalmente as classes não estão muito desbalanceadas.\n",
    "\n",
    "conclusão\n",
    "Encrontramos um ponto crítico de possível melhoria para futura:\n",
    "\n",
    "1. Ponto crítico de melhoria, para os recall em classe de pessoas doentes, positivas para ataque. tivemos recall = 62& ou seja deixou de predizer corretamente 38% dos doentes, o que pode ser fatal. \n",
    "1.1 Sugestão de solução: melhorar o balanceamento de pessoas doentes, ou seja, aumentar a quantidade de doentes na amostra de treinamento do modelo. Já para deixar os padrões de pessoas doentes, precisamos confirmar se temos mais atributos que podem explicam melhor a classe 1(pessoas doentes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTEXTO PARA explicação:\n",
    "TP (True Positive): O modelo previu 1 e o rótulo real era 1 → previsão correta de positivo.\n",
    "\n",
    "TN (True Negative): O modelo previu 0 e o rótulo real era 0 → previsão correta de negativo.\n",
    "\n",
    "FP (False Positive): O modelo previu 1 mas o rótulo real era 0 → \"falso alarme\".\n",
    "\n",
    "FN (False Negative): O modelo previu 0 mas o rótulo real era 1 → \"falha na deteção\".\n",
    "\n",
    "\n",
    "| Real \\ Previsto | 0 (Previsto) | 1 (Previsto) |\n",
    "|-----------------|--------------|--------------|\n",
    "| 0 (Real)        | TN           | FP           |\n",
    "| 1 (Real)        | FN           | TP           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resultado do classification_report\n",
    "Relatório de Classificação:\n",
    "\n",
    "classe 0 => Saudável (falso)\n",
    "\n",
    "classe 1 => doente (positivo)\n",
    "\n",
    "# Sem oneHotEncoder\n",
    "\n",
    "| Classe        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| **0**         | 0.73      | 0.76   | 0.75     | 68      |\n",
    "| **1**         | 0.66      | 0.62   | 0.64     | 50      |\n",
    "| **Accuracy**  | —         | —      | 0.70     | 118     |\n",
    "| **Macro Avg** | 0.70      | 0.69   | 0.69     | 118     |\n",
    "| **Weighted Avg** | 0.70  | 0.70   | 0.70     | 118     |\n",
    "\n",
    "\n",
    "# Com oneHotEncoder\n",
    "| Classe        | Precision | Recall | F1-Score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| **0**         | 0.97      | 0.87   | 0.91     | 68      |\n",
    "| **1**         | 0.84      | 0.96   | 0.90     | 50      |\n",
    "| **Accuracy**  | —         | —      | 0.91     | 118     |\n",
    "| **Macro Avg** | 0.90      | 0.91   | 0.91     | 118     |\n",
    "| **Weighted Avg** | 0.91  | 0.91   | 0.91     | 118     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prever a condição de um novo paciente\n",
    "*com valores fictícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo os campos:\n",
    "| Campo           | Descrição                                                    | Exemplo | Tipo        | Por extenso / Significado                                  |\n",
    "|-----------------|--------------------------------------------------------------|---------|-------------|-------------------------------------------------------------|\n",
    "| Age             | Idade do paciente                                            | 55      | numérica    | 55 anos                                                     |\n",
    "| Sex             | Sexo do paciente                                             | M       | categórica  | M = masculino, F = feminino                                 |\n",
    "| ChestPainType   | Tipo de dor no peito                                         | ATA     | categórica  | ATA = angina atípica; ASY = assintomático; TA = típica; NAP = não anginosa |\n",
    "| RestingBP       | Pressão arterial em repouso (mm Hg)                           | 130     | numérica    | 130 mm Hg                                                   |\n",
    "| Cholesterol     | Colesterol sérico total (mg/dl)                               | 245     | numérica    | 245 mg/dl                                                   |\n",
    "| FastingBS       | Glicemia em jejum > 120 mg/dl?                                | 0       | categórica  | 0 = normal; 1 = alta (acima de 120 mg/dl)                   |\n",
    "| RestingECG      | Resultado do eletrocardiograma em repouso                     | ST      | categórica  | Normal / Anomalia ST-T / Hipertrofia Ventricular Esquerda  |\n",
    "| MaxHR           | Frequência cardíaca máxima atingida                           | 150     | numérica    | 150 bpm                                                     |\n",
    "| ExerciseAngina  | Angina induzida por exercício                                 | N       | categórica  | N = não apresentou; Y = apresentou angina                   |\n",
    "| Oldpeak         | Depressão do segmento ST por exercício                        | 1.0     | numérica    | Depressão ST de 1.0 mm                                      |\n",
    "| ST_Slope        | Inclinação do segmento ST no pico do exercício                | Down    | categórica  | Up = subida; Flat = plano; Down = descida                   |\n",
    "| HeartDisease    | Presença de doença cardíaca (TARGET)                          | 1       | categórica  | 1 = possui doença cardíaca; 0 = não possui                  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reaplicar o modelo filtrando as colunas originais e usando as colunas tratadas dos outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knowledge\n",
    "* Entre pessoas experientes em Data Stewardship (com foco em governança + explicabilidade + robustez), a opção mais recomendada costuma ser:\n",
    "\n",
    "✅ Usar _capped + _outlier_flag e NÃO usar as colunas originais (não capped) no modelo.\n",
    "(equivalente à tua Opção A, mas “sem duplicar” a mesma variável duas vezes.)\n",
    "\n",
    "Por quê essa é a mais defendida?\n",
    "\n",
    "- Robustez: o modelo não “explode” por valores extremos.\n",
    "- Explicabilidade: a flag diz claramente “esse caso era extremo”.\n",
    "- Governança: fica transparente o que foi tratado e quando (útil para auditoria e stakeholders).\n",
    "- Menos ruído: evitar manter original + capped juntos reduz colinearidade e confusão na interpretação.\n",
    "\n",
    "Quando eu NÃO recomendaria essa opção?\n",
    "- Se o outlier é um evento raro porém super informativo (ex.: fraude, picos reais) e você quer que o modelo “sinta” a magnitude total. Aí você pode manter o original também — mas isso é uma decisão consciente, não padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomendado: usar _capped + _outlier_flag e remover as colunas originais contínuas\n",
    "target = \"HeartDisease\"\n",
    "orig_continuous = [\"RestingBP\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\"]\n",
    "\n",
    "capped_cols = [c for c in df_encoded.columns if c.endswith(\"_capped\")]\n",
    "flag_cols  = [c for c in df_encoded.columns if c.endswith(\"_outlier_flag\")]\n",
    "\n",
    "# base = tudo que não é target, não é original contínua, não é capped/flag\n",
    "base_cols = [\n",
    "    c for c in df_encoded.columns\n",
    "    if c != target\n",
    "    and c not in orig_continuous\n",
    "    and not c.endswith(\"_capped\")\n",
    "    and not c.endswith(\"_outlier_flag\")\n",
    "]\n",
    "\n",
    "X = df_encoded[base_cols + capped_cols + flag_cols]\n",
    "y = df_encoded[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classificação \n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict: previsões com os dados de teste\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")    \n",
    "plt.title(\"Matriz de Confusão\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Real \\ Previsto | 0 (Previsto) | 1 (Previsto) |\n",
    "|-----------------|--------------|--------------|\n",
    "| 0 (Real)        | TN           | FP           |\n",
    "| 1 (Real)        | FN           | TP           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matriz de Confusão:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRelatório de Classificação:\\n\" , classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTIMIZAÇÃO GRID SEARCH CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Hiper parametros\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": [3, 5, 7, 9, 11],\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    \"knn__metric\": [\"euclidean\", \"manhattan\"]\n",
    "}\n",
    "\n",
    "# otimização de vizinhos\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Melhores parâmetros:\", grid.best_params_)\n",
    "print(f\"Melhor accuracy (validação): {grid.best_score_:.3f} \")\n",
    "\n",
    "# best_model = grid.best_estimator_\n",
    "# y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# accuracy_score: percentagem de classificações corretas\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy no teste: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOVA PREDIÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 0) Novo paciente (formato original)\n",
    "novo_paciente = pd.DataFrame([{\n",
    "    \"Age\": 58,\n",
    "    \"Sex\": \"M\",\n",
    "    \"ChestPainType\": \"ATA\",\n",
    "    \"RestingBP\": 138,\n",
    "    \"Cholesterol\": 240,\n",
    "    \"FastingBS\": 0,\n",
    "    \"RestingECG\": \"ST\",\n",
    "    \"MaxHR\": 160,\n",
    "    \"ExerciseAngina\": \"N\",\n",
    "    \"Oldpeak\": 1.4,\n",
    "    \"ST_Slope\": \"Flat\"\n",
    "}])\n",
    "\n",
    "\n",
    "# 1) One-hot encoding (usar o MESMO encoder já fitado)\n",
    "df_categorical = novo_paciente.select_dtypes(exclude=[\"number\"])\n",
    "encoded_arr = encoder.transform(df_categorical)\n",
    "\n",
    "encoded_df_new = pd.DataFrame(\n",
    "    encoded_arr,\n",
    "    columns=encoder.get_feature_names_out(df_categorical.columns),\n",
    "    index=novo_paciente.index\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Numéricas + capped/flags (usar os MESMOS limites iqr_limits do treino)\n",
    "df_numeric = novo_paciente.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "for col in [\"RestingBP\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\"]:\n",
    "    low, high = iqr_limits[col]\n",
    "    df_numeric[f\"{col}_outlier_flag\"] = ((df_numeric[col] < low) | (df_numeric[col] > high)).astype(int)\n",
    "    df_numeric[f\"{col}_capped\"] = df_numeric[col].clip(lower=low, upper=high)\n",
    "\n",
    "df_numeric_final = df_numeric[[\n",
    "    \"Age\", \"FastingBS\",\n",
    "    \"RestingBP_capped\", \"Cholesterol_capped\", \"MaxHR_capped\", \"Oldpeak_capped\",\n",
    "    \"RestingBP_outlier_flag\", \"Cholesterol_outlier_flag\", \"MaxHR_outlier_flag\", \"Oldpeak_outlier_flag\"\n",
    "]]\n",
    "\n",
    "\n",
    "# 3) Montar X do novo paciente e alinhar com as colunas do treino\n",
    "novo_X = pd.concat([df_numeric_final, encoded_df_new], axis=1)\n",
    "\n",
    "# alinhar com as colunas que o modelo espera (X_train.columns)\n",
    "novo_X = novo_X.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "\n",
    "# 4) Escalar com o MESMO scaler (alinhar com as colunas do scaler)\n",
    "if hasattr(scaler, \"feature_names_in_\"):\n",
    "    cols_scaler = list(scaler.feature_names_in_)\n",
    "else:\n",
    "    cols_scaler = list(X_train.columns)  # fallback: use a ordem do treino\n",
    "\n",
    "novo_X_for_scaler = novo_X.reindex(columns=cols_scaler, fill_value=0)\n",
    "novo_X_scaled = scaler.transform(novo_X_for_scaler)\n",
    "\n",
    "\n",
    "# 5) Predição com o modelo treinado (GridSearchCV)\n",
    "pred_novo = grid.predict(novo_X_scaled)\n",
    "\n",
    "print(pred_novo[0])\n",
    "print(f\"Risco previsto: {'Doente' if pred_novo[0] == 1 else 'Saudável'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
